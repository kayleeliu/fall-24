# Privacy and Fairness
## 1. Introduction & Motivation
Differential privacy (DP) is a technique for training machine learning models while limiting information leakage regarding individual data points in the training set. Implementing DP often comes at the expense of model accuracy. Yet, the impact of this trade-off on the accuracy across various dataset classes remains ambiguous. Research indicated in study [3] reveals that when neural networks are trained with differentially private stochastic gradient descent (DP-SGD), the accuracy loss is disproportionately higher for underrepresented classes and subgroups. This suggests that the cost of achieving differential privacy in machine learning models is unevenly distributed, particularly affecting the minority groups within the data.

More generally, DP’s impact on fairness has gained increasing attention as it becomes more widely used. With this influx of research comes varying results about the interaction between privacy and fairness. In order to understand the state of current research and areas in need of future work, [1] conducts a survey covering the breadth of perspectives, ideas, and concepts discussed from numerous works. As a more focused study, [2] considers the tensions between DP and statistical notions of fairness, particularly Equality of False Positives and Equality of False Negatives. DP learning systems may induce bias and unfair outcomes for certain groups of individuals, which can have significant societal and economic impacts. The cause of this bias is largely understudied and not fully understood; study [4] aims to address this knowledge gap through examination of empirical risk minimization with respect to fairness.  


## 2. Methods
### 2.1 Differential Privacy and Fairness in Decisions and Learning Tasks
The authors in [1] conduct a survey on recent work in the intersection of differential privacy (DP) and fairness. They attempt to clarify notions of the interaction between DP and fairness while also reviewing the current state of research in the area. There is a focus on decision tasks and learning tasks, where the effects induced by some DP mechanism on the outputs of some task of interest involving sensitive information are highlighted. Two views on the alignment of privacy and fairness, with one seeing them as opposed and the other aligned, are investigated. Furthermore, the reasoning behind the disparate impacts of privacy on fairness are reviewed. Finally, mitigations for the issues that arise due to DP are in these types of tasks are discussed, alongside research challenges which must be addressed to have a full understanding of the trade-offs between privacy, fairness, and accuracy.

### 2.2 On the Compatibility of Privacy and Fairness
To evaluate DP, the authors of [2] consider two types of neighboring databases that differ by a single entry: 1) a finite sample and 2) a distribution D over X. For the purposes of this study, equal opportunity is used as a notion of exact fairness for the second type of neighboring distribution. Equal opportunity is defined as the Equality of False Negatives (EFN), requiring the false positive rates to be equal on neighboring groups. For the first, finite type of neighboring distribution, the authors consider approximate fairness instead of exact fairness, as achieving exact fairness is impossible for DP, which they mathematically prove in their findings. 

### 2.3 Disparate Impact of Differential Privacy on Model Accuracy
To assess disparate impact, the authors of study [3] employed accuracy parity as their metric. Through a series of experiments, they evaluated the model's performance, paying special attention to its accuracy both on the imbalanced class and across various imbalanced subgroups within the input domain. Their investigation spanned the accuracy disparity between Differential Privacy (DP) and Non-DP models across a range of classification tasks. These tasks encompassed gender and age classification using facial images, sentiment analysis of tweets, species identification in nature images, and federated learning for a language model. Additionally, the study delved into how different hyperparameters affect model performance, utilizing the MNIST dataset for this analysis. These hyperparameters included batch size, number of epochs, noise multiplier, and gradient clipping, offering insights into their impact on the model's efficacy.

### 2.4 Differentially Private Empirical Risk Minimization under the Fairness Lens
The authors of study [4] aim to gain a better understanding of risk minimization with respect to fairness, and focus on the problem of differentially private empirical risk minimization (ERM). They focus on two main DP learning methods: output perturbation and differentially private stochastic gradient descent (DP-SGD). 

Output perturbation is a DP paradigm where noise is added directly to the output of the computation, and the noise is calibrated to the function sensitivity. The authors mathematically formalize a number of observations with respect to fairness with respect to DP induced by output perturbation. 

DP-SGD is a valuable framework for DP ERM problems because it does not restrict focus on convex loss functions. Furthermore, the privacy analysis does not require optimality of model parameters. As with output perturbation, the authors mathematically formalize some important observations of DP-SGD. 


## 3. Key Findings
### 3.1 Differential Privacy and Fairness in Decisions and Learning Tasks
Rather than gathering results from an evaluation to draw conclusions, the survey [1] summarizes findings from other studies. From these other works, they compare different notions of fairness, particularly group and individual fairness. The authors state that there are two perspectives about privacy and fairness, which are that they are aligned and contrasting objectives. This is drawn from research that shows how individual privacy is a generalization of DP, and other works that observe  that the outputs of DP classifiers may create or exacerbate disparate impacts among groups of individuals. This is further explored by seeing why disparate impacts arise in decision tasks and learning tasks. The negative impacts of privacy towards fairness in decision tasks is attributed to two factors: shape of the decision problem and the presence of non-negativity constraints in post-processing steps. It was observed that tight privacy requirements (small privacy loss values) or non-linearities in the decision problem may lead to large disparate impacts, so that using DP to generate private inputs of decision problems will necessarily introduce fairness issues, even if the noise is unbiased. Post-processing which is used to reduce errors, also can introduce bias and fairness issues,  even simple non-negative steps. As for learning tasks, it is shown that privacy preserving models can introduce substantial fairness issues and this can be attributed to (1) the properties of the training data, specifically the input norms and distance to decision boundary, and (2) the model’s characteristics, such as gradient clipping and adding noise. The survey also discusses proposed strategies to mitigate fairness issues in the aforementioned tasks, such as distributing additional budget to target entities and creating a near-optimal projection operator that meets feasibility requirements of allotment problems and has less of a disparate impact on fairness. Other strategies focus on the DP-SGD framework, like associating a different clipping bound to each protected group. The authors highlight five existing challenges in this research, including developing a unified theoretical framework to characterize and reason about fairness issues in general decision tasks and understanding the link between privacy and fairness with model robustness. 

### 3.2 On the Compatibility of Privacy and Fairness
Evaluating the relationship between privacy and fairness, [2] considers the simplest possible task, grant their algorithm full distributional access to the underlying population, and ask only for non-trivial classification accuracy. The authors formally prove that DP and exact fairness are impossible to achieve with non-trivial accuracy for a classifier. Furthermore, the authors loosen exact fairness constraints and give promising results for optimizing privacy and approximate fairness. Specifically, they consider a more challenging environment where the algorithm only has sample access to the distribution, and it must agnostically learn a hypothesis for making decisions. Lastly, the authors recreates a Fair No-Regret Dynamics algorithm from a prior work, tweaking it to return a fair and accurate randomized classifier with high probability.

### 3.3 Disparate Impact of Differential Privacy on Model Accuracy
Study [3] revealed distinct disparities in the performance of Differential Privacy (DP) models compared to non-DP models across various tasks. In the task of gender classification from facial images, it was observed that the accuracy of DP models decreases significantly more for darker-skinned faces than for lighter-skinned ones. In the context of classifying tweets as either Standard American English (SAE) or African-American English (AAE), while all models almost perfectly identified the SAE subgroup, the accuracy for the AAE subgroup dropped substantially more in DP models than in non-DP models. In species classification, DP models approached the accuracy of non-DP models for well-represented classes but fell considerably short for smaller classes.

The study also explored the effects of hyperparameters on model performance using the MNIST dataset, demonstrating that clipping and noise introduction hindered model convergence. Specifically, the addition of noise, when its magnitude was comparable to the update vector, prevented clipped gradients from adequately updating the model in relation to underrepresented classes. Larger batch sizes were found to diminish the adverse effects of noise. Furthermore, the analysis showed that for both DP and non-DP models, accuracy is influenced by the size of the underrepresented group, generally improving as the class size increases.


### 3.4 Differentially Private Empirical Risk Minimization under the Fairness Lens
Study [4] focuses on the concept of excessive risk in developing a notion of fairness under private training, with a specific look into output perturbation and DP-SGD. For output perturbation, the authors determine that output perturbation may introduce unfairness when the loss functions of different groups differ in a substantial manner. More specifically, the local curvatures associated with the loss functions of these groups may be key to unfairness. 

For DP-SGD, the authors find that there are two main sources of disparate impact in DP-SGD training: gradient clipping and noise addition. When examining why gradient clipping causes unfairness, they find that there are three main factors that influence the clipping effect: Hessian loss, gradient values, and the Clipping bound. They note that unfairness with the average gradient norms of groups a and b and the clipping value C are related. Furthermore, group data with large input norms result in large gradient norms, leading to larger disproportionate impacts than groups with smaller input norms. For noise addition, the authors note that the difference in input norms is crucial to unfairness. Groups with larger input norms will have larger disproportionate impacts under private training than groups with smaller input norms. The main factor that affects noise for a group is the Hessian loss, which is controlled by the group's distance to the decision boundary and their input norm. 

Finally, the authors introduce a mitigation solution. This mitigation solution is designed to mitigate unfairness by equalizing factors responsible for excessive risk associated with clipping and excessive risk associated with noise addition. This solution comes from the observation that the excessive risk for a group can mainly be attributed to these two factors. 


## 4. Critical Analysis
### 4.1 Differential Privacy and Fairness in Decisions and Learning Tasks
The study is commendable in detailing both viewpoints on privacy and fairness in regards to them being aligned vs. contrasting objectives. This lays a good foundation for the following analysis about why they may oppose each other. Additionally, it contributes to the field of research by providing the main challenges and risks that arise when using differential privacy in ML and decision making tasks related to fairness, which can negatively affect already vulnerable groups in societal and economic sectors. However, the paper could benefit from a more comprehensive analysis of the trade-offs between privacy guarantees and fairness considerations in DP algorithms, as well as practical strategies for addressing these challenges in real-world applications.

### 4.2 On the Compatibility of Privacy and Fairness
[2] offers a rigorous theoretical contribution between DP and different notions of statistical fairness, such as Equality of False Positives and Equality of False Negatives, and showing the existence of a PAC learner that is private and approximately fair. Furthermore, the findings of this paper have practical implications for designing classifiers that aim to balance privacy and fairness concerns, which are important considerations in many real-world applications. However, the findings of this paper have a limited scope in terms of the types of classifiers and fairness measures considered. It might not cover all possible scenarios and may not generalize to all types of datasets and applications. Overall, the authors emphasize the need for transparency and accountability in algorithm design, while also highlighting the ethical responsibility of designers to consider the societal impact of their algorithms.

### 4.3 Disparate Impact of Differential Privacy on Model Accuracy
The study [3] examines the consequences of implementing differential privacy (DP) in machine learning models, particularly through Differentially Private Stochastic Gradient Descent (DP-SGD). It reveals a significant trade-off, where efforts to enhance privacy through DP mechanisms can exacerbate existing biases, disproportionately affecting underrepresented groups by reducing model accuracy more severely for these groups than for well-represented ones. This finding is pivotal as it challenges the perceived universal benefit of DP, highlighting a crucial ethical concern where privacy measures could unintentionally deepen disparities. The paper's main contribution lies in its empirical evidence across various datasets and models, illustrating this complex interaction between privacy, accuracy, and fairness. However, it stops short of proposing solutions to mitigate these effects, underscoring a vital area for future research to develop privacy-enhancing technologies that do not compromise fairness.

### 4.4 Differentially Private Empirical Risk Minimization under the Fairness Lens
The strength of study [4] lies in its comprehensive examination into the causes of the unfairness in differentially private learning. The study aims to take a wide view of the issue, attempting to understand the issue from numerous angles. 

The authors recognize some potential limitations in the paper. Their analysis into gradient clipping contains the requirement that ERM losses are smooth and convex. The authors mention that such assumptions are common, but also that exploration of the non-convex case could be fascinating. Additionally, the proposed mitigation solution in the paper negatively affects training runtime, which leaves the design of a more efficient solution as a potential future area of improvement. 


## References
[1]. Differential Privacy and Fairness in Decisions and Learning Tasks: A Survey Fioretto et al, 2022.
[2]. On the Compatibility of Privacy and Fairness Cummings et al. 2019
[3]. Differential Privacy Has Disparate Impact on Model Accuracy Bagdasaryan 2019
[4]. Differentially Private Empirical Risk Minimization under the Fairness Lens Tran et al 2021